<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>Lab 5. Bias and Ethics (35 pts)</title><style>
/* cspell:disable-file */
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	text-indent: -1.7em;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

.simple-table {
	margin-top: 1em;
	font-size: 0.875rem;
	empty-cells: show;
}
.simple-table td {
	height: 29px;
	min-width: 120px;
}

.simple-table th {
	height: 29px;
	min-width: 120px;
}

.simple-table-header-color {
	background: rgb(247, 246, 243);
	color: black;
}
.simple-table-header {
	font-weight: 500;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	max-height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, ui-serif, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; }
.pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; }
.pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; }
.pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; }
.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; }
.pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; }
.pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; }
.pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; }
.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.highlight-default {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.highlight-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.highlight-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.highlight-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.highlight-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.highlight-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.highlight-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.highlight-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.highlight-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.highlight-gray_background {
	background: rgba(241, 241, 239, 1);
}
.highlight-brown_background {
	background: rgba(244, 238, 238, 1);
}
.highlight-orange_background {
	background: rgba(251, 236, 221, 1);
}
.highlight-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.highlight-teal_background {
	background: rgba(237, 243, 236, 1);
}
.highlight-blue_background {
	background: rgba(231, 243, 248, 1);
}
.highlight-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.highlight-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.highlight-red_background {
	background: rgba(253, 235, 236, 1);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.block-color-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.block-color-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.block-color-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.block-color-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.block-color-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.block-color-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.block-color-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.block-color-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.block-color-gray_background {
	background: rgba(241, 241, 239, 1);
}
.block-color-brown_background {
	background: rgba(244, 238, 238, 1);
}
.block-color-orange_background {
	background: rgba(251, 236, 221, 1);
}
.block-color-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.block-color-teal_background {
	background: rgba(237, 243, 236, 1);
}
.block-color-blue_background {
	background: rgba(231, 243, 248, 1);
}
.block-color-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.block-color-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.block-color-red_background {
	background: rgba(253, 235, 236, 1);
}
.select-value-color-pink { background-color: rgba(245, 224, 233, 1); }
.select-value-color-purple { background-color: rgba(232, 222, 238, 1); }
.select-value-color-green { background-color: rgba(219, 237, 219, 1); }
.select-value-color-gray { background-color: rgba(227, 226, 224, 1); }
.select-value-color-opaquegray { background-color: rgba(255, 255, 255, 0.0375); }
.select-value-color-orange { background-color: rgba(250, 222, 201, 1); }
.select-value-color-brown { background-color: rgba(238, 224, 218, 1); }
.select-value-color-red { background-color: rgba(255, 226, 221, 1); }
.select-value-color-yellow { background-color: rgba(253, 236, 200, 1); }
.select-value-color-blue { background-color: rgba(211, 229, 239, 1); }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="fab113f3-8984-4610-a9d4-b9c1c017b911" class="page serif"><header><h1 class="page-title">Lab 5. Bias and Ethics (35 pts)</h1></header><div class="page-body"><h3 id="2560a369-cf8b-404d-841b-17cb332c6c23" class=""><mark class="highlight-blue">OIDD 255X, Tambe</mark></h3><hr id="0432303f-7125-45e5-b60f-e095d7bccd34"/><p id="c49d70d9-3944-4143-bd73-93b2058122e7" class="">
</p><figure id="0b20cb3d-1f30-47df-9e44-44c8d5665736" class="image"><a href="Lab%205%20Bias%200b20c/cash-bail-header.jpg"><img style="width:480px" src="Lab%205%20Bias%200b20c/cash-bail-header.jpg"/></a></figure><h2 id="bba911d6-b69e-4601-baed-ce3e9cbbbbf4" class="">Deliverables</h2><p id="43e1d1ca-a233-42cd-8903-e8839fb0f9c8" class="">For this assignment, you are encouraged to discuss with others, but <strong>please complete the assignment and submit it on your own</strong>. Please submit a response document (e.g. a Word or Google Doc) with answers to each of the questions below. Assignments are to be submitted through Canvas. Please see Canvas for the due date.</p><h2 id="6b048bf0-4cbc-4b89-92c0-8c52e27b44cf" class="">Introduction</h2><p id="3ec2480c-5132-4593-b3ee-2a2195a6d5ee" class="">For background, read through <a href="https://www.theatlantic.com/ideas/archive/2019/06/should-we-be-afraid-of-ai-in-the-criminal-justice-system/592084/">Should we be afraid of AI in the Criminal Justice System?</a> The data needed to complete the assignment are <a href="https://upenn.box.com/s/8fs0qoguhxa7sb3gffa9ybodgmcprp94">available here</a>. This data set reports <a href="https://www.findlaw.com/criminal/criminal-procedure/arraignment.html">arraignment outcomes</a> for defendants from NYC. The data dictionary to help you understand the different fields can be found <a href="https://www.nycourts.gov/legacypdfs/court-research/PretrialReleaseDataDictionaryWeb.pdf">here</a>.</p><p id="0ba77f50-0901-4055-a3d0-000fd97ffdb8" class="">Imagine you are a <strong>product manager</strong> for a new product being designed to be sold into the criminal justice system (i.e. systems of courts around the US). You know that your potential future clients — courts — are sensitive to issues of <strong>fairness</strong> and <strong>transparency, </strong>particularly because the Department of Justice regularly audits districts to evaluate their judicial outcomes.</p><p id="24a49787-ab17-460d-9d96-0236b7117e52" class="">The product you are managing is intended to predict whether or not a defendant should be<strong> </strong><a href="https://www.law.cornell.edu/wex/release_on_one%27s_own_recognizance"><strong>released on recognizance</strong></a>. A correct “released on recognizance” decision requires fewer resources from the courts, and of course is a good outcome for defendants as well. Eventually, you would like to market the product as a tool to help legal systems rapidly make recommendations at scale, even in the absence of a judge.</p><p id="60935daa-c8ce-4393-b242-a7bb9c9b6c0e" class="">This lab requires a fair amount of work in WEKA and Excel (or you can use Python or R, if you prefer), but keep your eyes on the “big picture”, which is the relationships between data, bias, and transparency. If you choose Excel, <a href="https://blog.hubspot.com/marketing/how-to-create-pivot-table-tutorial-ht">Pivot Tables</a> can make the work go more quickly and are generally a good skill to learn, particularly for those of you headed to finance.</p><hr id="16ba18a0-174a-4768-a7d2-3ce41834a966"/><h2 id="3fed6ec0-7882-4791-8f2d-ea0fdfe0a760" class=""><mark class="highlight-blue">Part 1. Fairness in algorithms (9 pts)</mark></h2><p id="c0859a16-1f4c-4fa9-949d-416f08a2454b" class="">During the product development process, you start by running a simple decision tree on the training data. To do so, limit the variables in the data set to only the following:</p><figure id="57d92217-67fb-4491-bb8e-27d4ec5ec5a8" class="image"><a href="Lab%205%20Bias%200b20c/Screen_Shot_2022-03-27_at_10.05.39_PM.png"><img style="width:288px" src="Lab%205%20Bias%200b20c/Screen_Shot_2022-03-27_at_10.05.39_PM.png"/></a></figure><p id="7cb104fd-ed7d-4e96-a826-6b38b28c0021" class="">Choose a 66% percentage split for the data and a <strong>REPTree classifier</strong> (which is a type of decision tree). </p><figure id="f9134f43-c3e5-4f7f-a4c1-5c63a2b8163b" class="image"><a href="Lab%205%20Bias%200b20c/Screen_Shot_2022-03-27_at_10.09.32_PM.png"><img style="width:240px" src="Lab%205%20Bias%200b20c/Screen_Shot_2022-03-27_at_10.09.32_PM.png"/></a></figure><p id="47879dfd-a97d-4ff2-ad15-e635a7b0c0d9" class="">
</p><p id="607ba09c-a307-4b2d-bf56-387b4c17066b" class="">For the following questions, you will need to export the test data predictions, along with the demographic attributes for each row and then analyze the data in Excel, R, or Python. As you may recall from an earlier lab, you can choose “More Options” in WEKA and “Output Predictions” to have WEKA generate a file of predictions using the test data. In the “attributes” box, you can also specify if you would also like it to place feature values — such as gender and race — in the output file along with the predictions. </p><p id="5caa3d31-19ec-47de-aec7-7ce2f87c25cb" class="">The following questions ask you to evaluate the classifier according to various fairness criteria. Metrics for any two subgroups are never exactly equal, so for each of the metrics below, use the standard that the metric is “satisfied” if the numbers for the two different subgroups are<strong> within 5% of one another.</strong></p><h3 id="653079a8-f5ce-4e33-a586-cadbbf11d998" class=""><mark class="highlight-red">Question 1A</mark></h3><p id="a4613072-9d4c-428c-a811-8971299549a7" class="">For <strong>male</strong> and <strong>female</strong> gendered defendants, evaluate whether predictions on the test data satisfy the following <a href="https://afraenkel.github.io/fairness-book/content/05-parity-measures.html">fairness metrics</a> (note: you can ignore the 80% thresholds in the link; the link is being made available only so you can interpret different fairness concepts):<div class="indented"><ol type="1" id="9c165fad-e21d-4a7e-b736-165b7e252cc2" class="numbered-list" start="1"><li><strong>Demographic parity</strong></li></ol><ol type="1" id="5ef10d23-3f3f-4be7-aa5d-debf2f3cbc4b" class="numbered-list" start="2"><li><strong>Predictive parity</strong></li></ol><ol type="1" id="4ff042f0-e0e0-4623-bfff-3c53358d8236" class="numbered-list" start="3"><li><strong>Accuracy parity</strong></li></ol></div></p><h3 id="b74f2f6b-2195-4733-ae2d-0145bf488626" class=""><mark class="highlight-red">Question 1B</mark></h3><p id="9b016a4e-0ab1-4898-8fc9-c691dedca144" class="">In these data, white and black defendants are the two largest racial groups. For <strong>white</strong> and <strong>black</strong> defendants <span style="border-bottom:0.05em solid">only</span>, evaluate whether the predictions on the test data set satisfy the following fairness metrics:<div class="indented"><ol type="1" id="824d4080-0cef-4d76-ac92-e868ea22a1b8" class="numbered-list" start="1"><li><strong>Demographic parity</strong></li></ol><ol type="1" id="bf27446f-d2e0-4a05-afa1-89a882d9317e" class="numbered-list" start="2"><li><strong>Predictive parity</strong></li></ol><ol type="1" id="ac4dd7d0-f937-4591-b3c4-86fba3cd03e0" class="numbered-list" start="3"><li><strong>Accuracy parity</strong></li></ol></div></p><h3 id="b08a5315-b0d1-451c-ab26-1707e4c9fd02" class=""><mark class="highlight-red">Question 1C</mark></h3><p id="9e3cbf5c-2587-4c9f-834f-a3123b3705f4" class="">The data scientists on your team suggest that dropping gender or race variables can help to address this type of bias. This approach is known as “fairness-through-unawareness”. Consider a model that drops <span style="border-bottom:0.05em solid">gender</span> variables.  For this new model, are the following metrics satisfied for the gender subgroups? If not, are the outcomes closer to being fair than they were in Question 1A? (Note: you cannot output Weka predictions that contain Gender since you do not include it in the model, but you can cut and paste the column from the output prediction files produced in 1A or 1B).<div class="indented"><ol type="1" id="ad15a8ab-76b5-42b4-bbe7-591dc7b596ad" class="numbered-list" start="1"><li><strong>Demographic parity</strong></li></ol><ol type="1" id="387749a9-dbe7-40c7-a9e2-e41ea3afd959" class="numbered-list" start="2"><li><strong>Predictive parity</strong></li></ol><ol type="1" id="f381f8cd-4a8d-4dcf-a6f8-2cb99fa93f88" class="numbered-list" start="3"><li><strong>Accuracy parity</strong></li></ol></div></p><hr id="a545f001-40c8-4632-a3a6-136c5ee69796"/><h2 id="f06e3910-0dc4-49cd-9aaf-d86dff6d93fc" class=""><mark class="highlight-blue">Part 2. De-biasing approaches (6 pts)</mark></h2><p id="80c3db99-59b1-4732-b0ca-0443e240a013" class="">One way you might deal with bias in algorithms is by “<a href="https://www.ibm.com/docs/en/cloud-paks/cp-data/4.0?topic=insights-debiasing-options">de-biasing</a>” the training data. You decide with your team to try and apply de-biasing methods to alleviate biases based in data. </p><h3 id="36fc3da0-b621-40dc-947c-648e50cc38b1" class=""><mark class="highlight-red">Question 2A</mark></h3><p id="b2fed9bf-b13c-4c04-ac41-1b4b0e79ee57" class="">Using Excel or WEKA, alter the <em>labels</em> (i.e. the final decision) on the <span style="border-bottom:0.05em solid"><strong>training</strong></span> data to improve demographic parity between men and women in the test output. You do not have to “satisfy” demographic parity conditions according to the definitions above, but simply try to show that the results improve from a demographic parity perspective after your changes. Describe your steps and report demographic parity in the test data <strong>before</strong> and <strong>after</strong> your changes. (2 pts)</p><h3 id="83805a16-be15-49ec-bcc8-333efbe9f53f" class=""><mark class="highlight-red">Question 2B</mark></h3><p id="9828f64b-fccf-47b8-bd15-6a8e44ef1a76" class="">Using Excel or WEKA, alter the <em>features</em> (i.e. the gender column) on the <span style="border-bottom:0.05em solid"><strong>training data</strong></span> in a way that improves demographic parity in the test output. Again, you do not have to “satisfy” demographic parity conditions according to the definitions above, but simply try to show that there is greater demographic parity after your changes. Describe your steps and report demographic parity in the test data <strong>before</strong> and <strong>after</strong> your changes. (2 pts)</p><h3 id="a0f06926-50c1-47a0-8dbe-4164e30a1ef2" class=""><mark class="highlight-red">Question 2C</mark></h3><p id="5c637ad4-8db4-476f-9f5f-efad3d2e7590" class="">It is often said that for machine learning, “fairness comes at a price”. Create a table reporting the accuracy of the “de-biased” models and the accuracy of the original model run on the unaltered data (Part 1). (2 pts)</p><p id="c87434a4-002b-4cdd-afae-e0c5540d2eb7" class="">
</p><hr id="d1ce8c25-9006-440e-9041-a4027b9f7e8f"/><h2 id="89f4ceb0-bda7-46c8-a489-31b3d8ce878c" class=""><mark class="highlight-blue">Part 3. Explainability (10 pts)</mark></h2><p id="04aef83b-e743-4c87-b41b-5fa97ac35616" class="">In part based on the concerns associated with potential bias, you plan to position your product as a decision <em>augmentation</em> tool. This tool is intended to make recommendations to judges — not replace them — although they will continue to have the final say. Judges are only likely to accept the tool if they can understand the factors that are going into the recommendations.</p><h3 id="a1f43474-32bf-4501-9308-01496ad7767b" class=""><mark class="highlight-red">Question 3A</mark></h3><p id="28d01953-f972-47b1-afcd-89e41922873a" class="">Generate and paste (or screenshot) the <span style="border-bottom:0.05em solid">decision tree</span> generated by your original <strong>REPTree</strong> model. This tree can be viewed and captured by “right clicking” on the results buffer after running the model (note the Visualize Tree option). For this question, it does not matter if the tree is messy and illegible. (3 pts)</p><h3 id="a3e626d7-aa4c-4ed9-8f3e-e5c504774256" class=""><mark class="highlight-red">Question 3B</mark></h3><p id="a071e2e7-4701-4b65-b546-17f3ab3e0ea4" class="">To create a more <em>explainable</em> model, <span style="border-bottom:0.05em solid">limit</span> the tree depth to only 2 levels and rerun the model. Generate and paste (or screenshot) the decision tree generated by your new, <strong>depth limited REPTree</strong> model. Using the new model, how might you succinctly express the role of race in the prediction model? To adjust the tree depth, left click on the box shown below, which reports the name of the model you are using. (3 pts)</p><figure id="011b06df-0b71-4fb8-abf4-86afa2e29755" class="image"><a href="Lab%205%20Bias%200b20c/Screen_Shot_2022-03-29_at_9.08.08_PM.png"><img style="width:336px" src="Lab%205%20Bias%200b20c/Screen_Shot_2022-03-29_at_9.08.08_PM.png"/></a></figure><h3 id="2b4f7e4d-aa52-495b-9aae-3b538c7f4ae9" class=""><mark class="highlight-red">Question 3C</mark></h3><p id="4aec7f55-d8ef-4736-97d4-a58a93229fe6" class="">To illustrate explainability of the model from 3B, choose a row from the test data and use the tree from 3B to explain in a sentence or two why it produces the recommendation that it does. (2 pts)</p><h3 id="397dd6b3-dad3-4cc1-b0dc-2e75df2c2efa" class=""><mark class="highlight-red">Question 3D</mark></h3><p id="7e515a32-34f9-404a-85a0-6801b541760c" class="">What is the cost of transparency in this example? Specifically, how much does model <strong>accuracy</strong> suffer when opting for the depth limited model that is easier to explain? (2 pts)</p><p id="16187a61-0e7b-4137-83e5-59b40324855d" class="">
</p><hr id="938143f6-84ff-4276-abeb-f10550d1d167"/><h2 id="8b45cd8e-b643-48f1-83b6-ebb80bcdfa5b" class=""><mark class="highlight-blue">Part 4. Governance and Ethics (10 pts)</mark></h2><p id="e873e9c8-2ad0-474b-a006-8845b5632567" class="">Now, assume the role of a <span style="border-bottom:0.05em solid"><strong>regulator</strong></span>. Your goal in this role is to ensure that algorithmic tools are not amplifying harmful biases in the judicial process.</p><p id="2dcffb29-5e31-4033-b5d7-844d27e714fa" class="">Imagine that the product described above has been put into use by the NYC judiciary system. The product is trained on their historical data — based on the decisions that their judges have made in the past — and it will continue to train on such data as they are produced by the court system.</p><h3 id="2697aabe-a008-448a-bdbc-a09e5514299f" class=""><mark class="highlight-red">Question 4A</mark></h3><p id="962ceb1a-0b0b-44dc-8c42-39ca27e76ba2" class="">As a DOJ <a href="https://oig.justice.gov/sites/default/files/2020-01/Analyst.pdf"><strong>analyst</strong></a><strong> </strong>working for the Inspector General’s office, imagine that you have to prioritize a SINGLE metric to mandate and enforce gender subgroup fairness. For this specific algorithmic context (i.e. recommending which defendants should be “released on recognizance”), which one would you prioritize and why? It need not be one of those evaluated above. Be specific and defend your decision thoroughly and specifically. (2 pts)</p><h3 id="74a9287b-b2e8-45e5-95eb-6f471651b0a3" class=""><mark class="highlight-red">Question 4B</mark></h3><p id="7236aeb9-5a50-4948-9af3-bebff441f9aa" class="">To ensure fairness among male and female subgroups in this system, what specific data protocols would you require the users of the algorithms generate and make public? Be specific about a) what fields the data should contain, b) how these data should be distributed, c) how often it should be published, d) what types of tests it would allow you to run, and e) who should be responsible for running these tests. (2 pts)</p><h3 id="3ae49d4b-63a0-4ce3-a11d-207a11f0f9ae" class=""><mark class="highlight-red">Question 4C</mark></h3><p id="61b9a18d-820c-4c7c-839a-1609dafefe4c" class="">Now, suppose an audit is conducted that finds that the algorithm in use is generating predictions that clearly violate the metric that you have selected. The tech company blames the courts, arguing that their judges are biased and producing biased training data, and the algorithm is simply encoding this bias. The courts blame the technology company, arguing that they are responsible for any algorithmic output. In your auditor’s role, who do you feel should take the lead in fixing this issue, and why? (2 pts)</p><h3 id="e2465568-015c-4760-b591-842a6ca1bfb3" class=""><mark class="highlight-red">Question 4D</mark></h3><p id="60bcf50b-7f76-4d49-b448-d7793b4fe8f5" class="">Although biased outcomes are clearly harmful, the absence of algorithmic tools has arguably led to a significant backlog in trials, with defendants spending too long in jail awaiting decisions (e.g. see <a href="https://fredfrankbailbonds.com/bail-bonds/how-long-can-you-spend-in-jail">this article</a>). In no more than a paragraph, explain your position on whether the use of algorithms in criminal justice as described above is likely to be more harmful or beneficial.</p><p id="36b2f7ad-66ff-4f5e-8c7f-0458dffc75d3" class="">
</p><p id="84991191-76fc-4c6e-b488-294f46d43278" class="">
</p><p id="b0f1c976-829a-4424-a9a6-8b6aa8adf737" class="">
</p><p id="7dabbe4f-3d6a-428b-ae6d-9ad7740e6a72" class="">
</p><p id="7cec1030-49a0-4458-8a0d-ea1822822216" class="">
</p><p id="830cad9f-6e37-4f1b-b3c2-58c63aae8b7c" class="">
</p><p id="99a5b244-aa64-4b76-864f-04056b905a55" class="">
</p><p id="881acf6f-da11-40a9-89fd-5dd662994b85" class="">
</p></div></article></body></html>